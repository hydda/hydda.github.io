---
layout: page
title: HYDDA
menu: home
---


# Introduction
Les workflows Big Data dans leurs exécutions ont des besoins variés en
ressources d’infrastructures. Certaines phases
particulièrement
intensives en terme de calcul peuvent être contraintes en temps
(exemple, entraînement cyclique des modèles de machine learning
avancé ou Deep Learning). D’autres phases sont plus lâches sur les
aspects ressources de calcul mais nécessitent une connectivité IoT accrue
et une gestion du multi-tenants simplifiée (exemple, ingestion élastique
des données au plus près des sources, système de visualisation multi
utilisateurs).

# Objectif principal
Le projet vise à fournir une réponse au meilleur rendement
cout/performance pour ces applications Big Data en s’appuyant sur une
infrastructure hybride HPC/Cloud et en masquant la complexité du
déploiement à l’utilisateur avec un accès unifié et un placement
intelligent. La puissance des technologies HPC pourra permettre
d’accélérer les phases les plus critiques des workflows Big Data
nécessitant un traitement intensif. La flexibilité du cloud permettra
d’héberger les phases nécessitant une connectivité facile et une grande
adaptabilité. Le mouvement de données entre les infrastructures,
particulièrement énergivore, sera optimisé.

<!-- Description du projet-->

Le projet vise à fournir une réponse au meilleur rendement
cout/performance pour ces applications Big Data en s’appuyant sur une
infrastructure hybride HPC/Cloud et en masquant la complexité du
déploiement à l’utilisateur avec un accès unifié et un placement
intelligent. La puissance des technologies HPC pourra permettre
d’accélérer les phases les plus critiques des workflows Big Data
nécessitant un traitement intensif. La flexibilité du cloud permettra
d’héberger les phases nécessitant une connectivité facile et une grande
adaptabilité. Le mouvement de données entre les infrastructures,
particulièrement énergivore, sera optimisé.

Avec le déluge de données provoqué par la digitalisation de la société, de
nouveaux utilisateurs avec des nouvelles applications de système
analytique avancé apparaissent avec des moyens financiers limités et
pour autant des contraintes fortes sur les performances pour un usage
opérationnel. Nous proposons de traiter trois cas exemples représentatifs
dans ce projet :

* Dans la santé avec la médecine de précision (cas d’usage ICO) où le
traitement médical personnalisé nécessite de mobiliser à la demande
et dans l’instant des moyens de calcul importants pour chaque patient
afin d’assister le praticien dans son diagnostic et l’établissement de
l’ordonnance médicale. Au vue de la situation financière des
organismes de santé en France, le facteur coût par patient doit
impérativement être optimisé.

* Dans l’industrie aéronautique avec la démultiplication des simulations
scientifiques produisant de grandes données, pour optimiser
notamment la productivité. Ces simulations requièrent des
infrastructures spécifiques qui sont intrinsèquement partagées. Le
passage à l’échelle de l’accès à ces moyens de traitement limités doit
être nécessairement optimisé.

* Dans l’industrie numérique où les centres de données toujours plus
grands et critiques ne sont plus gérables par des humains et
nécessitent la mise en place de systèmes intelligents capables
d’assister l’humain dans ses tâches de maintenance et prédire les
défaillances. Les nouvelles techniques d’apprentissage automatique
basées sur les réseaux de neurones profonds devraient permettre
d’obtenir des performances satisfaisantes pour envisager des mises en
production rapides. Cependant le coût de l’infrastructure doit
nécessairement être diminué pour obtenir un ROI acceptable.

Le projet a pour but de fournir une plateforme répondant à ces exigences
en combinant intelligemment les technologies HPC et cloud tout en
assurant un accès facile aux utilisateurs non experts de l’IT.
